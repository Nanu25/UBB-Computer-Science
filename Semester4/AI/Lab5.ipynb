{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f939f18-6ca5-453f-853b-5b6b84b847d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "BitAdder(\n",
      "  (model): Sequential(\n",
      "    (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model 2:\n",
      "BitAdderV2(\n",
      "  (model): Sequential(\n",
      "    (fc1): Linear(in_features=2, out_features=6, bias=True)\n",
      "    (Lrelu): LeakyReLU(negative_slope=0.01)\n",
      "    (fc2): Linear(in_features=6, out_features=3, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc3): Linear(in_features=3, out_features=2, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Model 3:\n",
      "BitAdderV3(\n",
      "  (model): Sequential(\n",
      "    (fc1): Linear(in_features=2, out_features=4, bias=True)\n",
      "    (tanh): Tanh()\n",
      "    (fc2): Linear(in_features=4, out_features=6, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (fc3): Linear(in_features=6, out_features=2, bias=True)\n",
      "    (sigmoid): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Model 1 - Epoch [0/1000], Loss: 0.2722\n",
      "Model 1 - Epoch [100/1000], Loss: 0.1894\n",
      "Model 1 - Epoch [200/1000], Loss: 0.1594\n",
      "Model 1 - Epoch [300/1000], Loss: 0.1412\n",
      "Model 1 - Epoch [400/1000], Loss: 0.0875\n",
      "Model 1 - Epoch [500/1000], Loss: 0.0173\n",
      "Model 1 - Epoch [600/1000], Loss: 0.0080\n",
      "Model 1 - Epoch [700/1000], Loss: 0.0049\n",
      "Model 1 - Epoch [800/1000], Loss: 0.0034\n",
      "Model 1 - Epoch [900/1000], Loss: 0.0025\n",
      "\n",
      "Model 2 - Epoch [0/1000], Loss: 0.2911\n",
      "Model 2 - Epoch [100/1000], Loss: 0.1719\n",
      "Model 2 - Epoch [200/1000], Loss: 0.0287\n",
      "Model 2 - Epoch [300/1000], Loss: 0.0117\n",
      "Model 2 - Epoch [400/1000], Loss: 0.0062\n",
      "Model 2 - Epoch [500/1000], Loss: 0.0038\n",
      "Model 2 - Epoch [600/1000], Loss: 0.0025\n",
      "Model 2 - Epoch [700/1000], Loss: 0.0018\n",
      "Model 2 - Epoch [800/1000], Loss: 0.0014\n",
      "Model 2 - Epoch [900/1000], Loss: 0.0011\n",
      "\n",
      "Model 3 - Epoch [0/1000], Loss: 0.2275\n",
      "Model 3 - Epoch [100/1000], Loss: 0.0063\n",
      "Model 3 - Epoch [200/1000], Loss: 0.0005\n",
      "Model 3 - Epoch [300/1000], Loss: 0.0002\n",
      "Model 3 - Epoch [400/1000], Loss: 0.0001\n",
      "Model 3 - Epoch [500/1000], Loss: 0.0001\n",
      "Model 3 - Epoch [600/1000], Loss: 0.0001\n",
      "Model 3 - Epoch [700/1000], Loss: 0.0000\n",
      "Model 3 - Epoch [800/1000], Loss: 0.0000\n",
      "Model 3 - Epoch [900/1000], Loss: 0.0000\n",
      "\n",
      "Model 1 Weights:\n",
      "model.fc1.weight: tensor([[-2.8035, -2.8033],\n",
      "        [ 0.1945, -0.4951],\n",
      "        [-1.3887, -1.3888],\n",
      "        [-0.6839, -0.1670]])\n",
      "model.fc1.bias: tensor([ 2.8034, -0.3643,  2.7749, -0.6814])\n",
      "model.fc2.weight: tensor([[-1.3735, -0.2681, -4.2414,  0.2806],\n",
      "        [-4.6572, -0.1823,  4.5319,  0.2768]])\n",
      "model.fc2.bias: tensor([ 2.3210, -2.9672])\n",
      "\n",
      "Model 2 Weights:\n",
      "model.fc1.weight: tensor([[ 1.2132,  0.9398],\n",
      "        [ 0.9999,  0.7911],\n",
      "        [ 0.9498,  1.2451],\n",
      "        [ 1.4828,  1.1781],\n",
      "        [-0.6546, -0.9198],\n",
      "        [ 0.6429,  0.9426]])\n",
      "model.fc1.bias: tensor([-1.3059e-03, -7.0260e-01, -6.1765e-01, -5.8880e-01,  2.0600e+00,\n",
      "        -4.0547e-01])\n",
      "model.fc2.weight: tensor([[-0.0073, -0.0836,  0.1188, -0.0541, -0.0360, -0.3484],\n",
      "        [ 0.1645,  1.6158,  0.9496,  1.0827, -1.0679,  0.4704],\n",
      "        [-1.0721, -2.1924, -1.3702, -1.4144,  2.4477, -0.3984]])\n",
      "model.fc2.bias: tensor([-0.0018, -0.5943,  0.8320])\n",
      "model.fc3.weight: tensor([[ 0.0407,  1.5217, -2.3773],\n",
      "        [-0.0419, -1.6687, -1.3066]])\n",
      "model.fc3.bias: tensor([-2.1490,  3.7132])\n",
      "\n",
      "Model 3 Weights:\n",
      "model.fc1.weight: tensor([[ 0.6945,  0.8904],\n",
      "        [ 0.8867,  0.9399],\n",
      "        [-1.1242, -0.9298],\n",
      "        [-1.6381, -1.7160]])\n",
      "model.fc1.bias: tensor([-1.0543, -1.0482,  1.3839,  0.5312])\n",
      "model.fc2.weight: tensor([[-0.0646,  1.1431, -0.7851, -0.6839],\n",
      "        [-0.9952, -1.1435,  0.9364,  1.5114],\n",
      "        [ 0.1834, -0.0295,  0.0867,  0.6531],\n",
      "        [-0.9100, -1.2420,  1.5076,  1.4992],\n",
      "        [-1.3502, -1.0051,  1.5353, -0.7598],\n",
      "        [ 0.9878,  1.1684, -1.5551, -0.9368]])\n",
      "model.fc2.bias: tensor([-0.1829,  0.6010, -0.2734,  0.3082,  1.4551,  0.1705])\n",
      "model.fc3.weight: tensor([[ 1.1397, -1.3039, -0.1385, -0.7088, -1.4229,  1.3799],\n",
      "        [-0.4394, -1.7396,  0.2025, -1.5511,  1.7714, -1.3434]])\n",
      "model.fc3.bias: tensor([-0.8217, -0.0913])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "class BitAdder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BitAdder, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(2, 4)),  # Input layer to hidden layer\n",
    "            ('relu1', nn.ReLU()),      # Activation function\n",
    "            ('fc2', nn.Linear(4, 2)),  # Hidden layer to output layer\n",
    "            ('sigmoid', nn.Sigmoid())  # Sigmoid activation for binary output\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class BitAdderV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BitAdderV2, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(2, 6)),  # Input layer to larger hidden layer\n",
    "            ('Lrelu', nn.LeakyReLU(0.01)),  # Activation function\n",
    "            ('fc2', nn.Linear(6, 3)),  # Additional hidden layer\n",
    "            ('relu', nn.ReLU()),      # Activation function\n",
    "            ('fc3', nn.Linear(3, 2)),  # Output layer\n",
    "            ('sigmoid', nn.Sigmoid())  # Sigmoid activation for binary output\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class BitAdderV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BitAdderV3, self).__init__()\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(2, 4)),\n",
    "            ('tanh', nn.Tanh()),\n",
    "            ('fc2', nn.Linear(4, 6)),\n",
    "            ('relu', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(6, 2)),\n",
    "            ('sigmoid', nn.Sigmoid())\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize models\n",
    "model1 = BitAdder()\n",
    "model2 = BitAdderV2()\n",
    "model3 = BitAdderV3()\n",
    "\n",
    "print(\"Model 1:\")\n",
    "print(model1)\n",
    "print(\"\\nModel 2:\")\n",
    "print(model2)\n",
    "print(\"\\nModel 3:\")\n",
    "print(model3)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.01)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.01)\n",
    "\n",
    "# Training data (all possible inputs and their sum as binary output)\n",
    "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "labels = torch.tensor([[0, 0], [0, 1], [0, 1], [1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Train models\n",
    "def train_model(model, optimizer, name):\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'{name} - Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "train_model(model1, optimizer1, \"Model 1\")\n",
    "print()\n",
    "train_model(model2, optimizer2, \"Model 2\")\n",
    "print()\n",
    "train_model(model3, optimizer3, \"Model 3\")\n",
    "\n",
    "# Display model weights\n",
    "def print_weights(model, name):\n",
    "    print(f\"\\n{name} Weights:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f'{name}: {param.data}')\n",
    "\n",
    "print_weights(model1, \"Model 1\")\n",
    "print_weights(model2, \"Model 2\")\n",
    "print_weights(model3, \"Model 3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8503deef-8dac-4f9e-a7ca-a575137e829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing RELU Activation ---\n",
      "Epoch [0/500], Loss: 0.2463\n",
      "Epoch [100/500], Loss: 0.1431\n",
      "Epoch [200/500], Loss: 0.1285\n",
      "Epoch [300/500], Loss: 0.1262\n",
      "Epoch [400/500], Loss: 0.1257\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.01802737 0.0241109 ] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.49664798 0.5003635 ] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.03156736 0.97221845] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.4966451 0.5003668] | Expected: [1. 0.]\n",
      "\n",
      "--- Testing TANH Activation ---\n",
      "Epoch [0/500], Loss: 0.2618\n",
      "Epoch [100/500], Loss: 0.0804\n",
      "Epoch [200/500], Loss: 0.0077\n",
      "Epoch [300/500], Loss: 0.0027\n",
      "Epoch [400/500], Loss: 0.0015\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.02230498 0.02544831] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.028559 0.977457] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.03812669 0.9649823 ] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.9606348  0.02700343] | Expected: [1. 0.]\n",
      "\n",
      "--- Testing LEAKY_RELU Activation ---\n",
      "Epoch [0/500], Loss: 0.2798\n",
      "Epoch [100/500], Loss: 0.0616\n",
      "Epoch [200/500], Loss: 0.0074\n",
      "Epoch [300/500], Loss: 0.0025\n",
      "Epoch [400/500], Loss: 0.0012\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.00319471 0.03812715] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.01934917 0.9616614 ] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.02231631 0.9709504 ] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.97777176 0.0271922 ] | Expected: [1. 0.]\n",
      "\n",
      "--- Testing ELU Activation ---\n",
      "Epoch [0/500], Loss: 0.2273\n",
      "Epoch [100/500], Loss: 0.0870\n",
      "Epoch [200/500], Loss: 0.0070\n",
      "Epoch [300/500], Loss: 0.0019\n",
      "Epoch [400/500], Loss: 0.0010\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.00035389 0.03125371] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.02154687 0.97574234] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.0294633 0.9715451] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.97819567 0.02315181] | Expected: [1. 0.]\n",
      "\n",
      "--- Testing SELU Activation ---\n",
      "Epoch [0/500], Loss: 0.2951\n",
      "Epoch [100/500], Loss: 0.1233\n",
      "Epoch [200/500], Loss: 0.0101\n",
      "Epoch [300/500], Loss: 0.0020\n",
      "Epoch [400/500], Loss: 0.0009\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.00110465 0.02654651] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.02112537 0.9727391 ] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.02089763 0.975694  ] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.9749351  0.02738631] | Expected: [1. 0.]\n",
      "\n",
      "--- Testing HARDSWISH Activation ---\n",
      "Epoch [0/500], Loss: 0.2349\n",
      "Epoch [100/500], Loss: 0.0615\n",
      "Epoch [200/500], Loss: 0.0025\n",
      "Epoch [300/500], Loss: 0.0008\n",
      "Epoch [400/500], Loss: 0.0004\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.00029091 0.02055221] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.01473285 0.98492634] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.01116673 0.97925425] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.9873849  0.01706433] | Expected: [1. 0.]\n",
      "\n",
      "--- Testing MISH Activation ---\n",
      "Epoch [0/500], Loss: 0.2432\n",
      "Epoch [100/500], Loss: 0.0409\n",
      "Epoch [200/500], Loss: 0.0026\n",
      "Epoch [300/500], Loss: 0.0009\n",
      "Epoch [400/500], Loss: 0.0005\n",
      "\n",
      "Predictions:\n",
      "[0. 0.] | Pred: [0.00085439 0.0183817 ] | Expected: [0. 0.]\n",
      "[0. 1.] | Pred: [0.01919683 0.979614  ] | Expected: [0. 1.]\n",
      "[1. 0.] | Pred: [0.01302302 0.9818579 ] | Expected: [0. 1.]\n",
      "[1. 1.] | Pred: [0.97642684 0.01854848] | Expected: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ActivationBitAdder(nn.Module):\n",
    "    def __init__(self, activation='relu'):\n",
    "        super(ActivationBitAdder, self).__init__()\n",
    "        \n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            act_func = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            act_func = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            act_func = nn.LeakyReLU(0.01)\n",
    "        elif activation == 'elu':\n",
    "            act_func = nn.ELU(alpha=1.0)\n",
    "        elif activation == 'selu':\n",
    "            act_func = nn.SELU()\n",
    "        elif activation == 'hardswish':\n",
    "            act_func = nn.Hardswish()\n",
    "        elif activation == 'mish':\n",
    "            act_func = nn.Mish()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "        \n",
    "        # Model architecture with chosen activation\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(2, 6)),  \n",
    "            ('activation1', act_func),      \n",
    "            ('fc2', nn.Linear(6, 2)),  \n",
    "            ('sigmoid', nn.Sigmoid())  # Keep sigmoid for binary output\n",
    "        ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_and_evaluate(activation):\n",
    "    print(f\"\\n--- Testing {activation.upper()} Activation ---\")\n",
    "    \n",
    "    \n",
    "    data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "    labels = torch.tensor([[0, 0], [0, 1], [0, 1], [1, 0]], dtype=torch.float32)\n",
    "\n",
    "    model = ActivationBitAdder(activation)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch [{epoch}/500], Loss: {loss.item():.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data)\n",
    "        print(\"\\nPredictions:\")\n",
    "        for inp, pred, exp in zip(data, predictions, labels):\n",
    "            print(f\"{inp.numpy()} | Pred: {pred.numpy()} | Expected: {exp.numpy()}\")\n",
    "\n",
    "activations = ['relu', 'tanh', 'leaky_relu', 'elu', 'selu', 'hardswish', 'mish']\n",
    "for act in activations:\n",
    "    train_and_evaluate(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2efe58-1cae-44b3-a8fb-a34d6d7dfe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/1500], Loss: 0.2518, Learning Rate: 0.010000\n",
      "Epoch [100/1500], Loss: 0.0004, Learning Rate: 0.005500\n",
      "Epoch [200/1500], Loss: 0.0002, Learning Rate: 0.001000\n",
      "Epoch [300/1500], Loss: 0.0001, Learning Rate: 0.002318\n",
      "Epoch [400/1500], Loss: 0.0001, Learning Rate: 0.001000\n",
      "Epoch [500/1500], Loss: 0.0001, Learning Rate: 0.002318\n",
      "Epoch [600/1500], Loss: 0.0000, Learning Rate: 0.005500\n",
      "Epoch [700/1500], Loss: 0.0000, Learning Rate: 0.001343\n",
      "Epoch [800/1500], Loss: 0.0000, Learning Rate: 0.001000\n",
      "Epoch [900/1500], Loss: 0.0000, Learning Rate: 0.001343\n",
      "Epoch [1000/1500], Loss: 0.0000, Learning Rate: 0.002318\n",
      "Epoch [1100/1500], Loss: 0.0000, Learning Rate: 0.003778\n",
      "Epoch [1200/1500], Loss: 0.0000, Learning Rate: 0.005500\n",
      "Epoch [1300/1500], Loss: 0.0000, Learning Rate: 0.007222\n",
      "Epoch [1400/1500], Loss: 0.0000, Learning Rate: 0.008682\n",
      "\n",
      "Model Predictions:\n",
      "Inputs | Predictions | Expected\n",
      "[0. 0.] | [0.00192856 0.00213916] | [0. 0.]\n",
      "[0. 1.] | [2.5185346e-04 9.9849236e-01] | [0. 1.]\n",
      "[1. 0.] | [4.2371241e-05 9.9900514e-01] | [0. 1.]\n",
      "[1. 1.] | [9.9850559e-01 2.0382477e-05] | [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Best Example(not to show)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class AdvancedBitAdder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedBitAdder, self).__init__()\n",
    "        \n",
    "        # More sophisticated architecture with residual connections\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(2, 8),  # Expanded input layer\n",
    "            nn.BatchNorm1d(8),  # Batch normalization for stability\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Residual block\n",
    "            nn.Sequential(\n",
    "                nn.Linear(8, 8),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8, 8),\n",
    "            ),\n",
    "            \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Xavier initialization for better weight distribution\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Create custom learning rate scheduler\n",
    "class CosineAnnealingWarmRestarts(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1):\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.current_period = T_0\n",
    "        self.next_restart = T_0\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        return [self.eta_min + (base_lr - self.eta_min) * \n",
    "                (1 + np.cos(np.pi * self.last_epoch / self.current_period)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "    \n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "        \n",
    "        if epoch >= self.next_restart:\n",
    "            self.current_period *= self.T_mult\n",
    "            self.next_restart += self.current_period\n",
    "        \n",
    "        super().step(epoch)\n",
    "\n",
    "# Prepare training data\n",
    "data = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "labels = torch.tensor([[0, 0], [0, 1], [0, 1], [1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = AdvancedBitAdder()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error instead of Cross Entropy\n",
    "\n",
    "# Custom learning rate scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2, eta_min=0.001)\n",
    "\n",
    "# Training loop with more advanced techniques\n",
    "epochs = 1500\n",
    "best_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(data)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass with gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimize\n",
    "    optimizer.step()\n",
    "    scheduler.step(epoch)\n",
    "    \n",
    "    # Early stopping mechanism\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        early_stop_counter = 0\n",
    "        # Optional: Save best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print( f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}, '\n",
    "              f'Learning Rate: {scheduler.get_lr()[0]:.6f}')\n",
    "    \n",
    "    # Early stopping if no improvement\n",
    "    if early_stop_counter > 300:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nModel Predictions:\")\n",
    "with torch.no_grad():\n",
    "    predictions = model(data)\n",
    "    print(\"Inputs | Predictions | Expected\")\n",
    "    for inp, pred, exp in zip(data, predictions, labels):\n",
    "        print(f\"{inp.numpy()} | {pred.numpy()} | {exp.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aaf483-3fb2-4b67-8114-4354acd6e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbdf0d3-cd64-478e-b7d3-e639929c5222",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
